<?php

declare(strict_types=1);

namespace Tolyan\Openai\ChatCompletion;

use Crell\Serde\Attributes\ClassSettings;
use Crell\Serde\Attributes\PostLoad;
use Crell\Serde\Renaming\Cases;
use Tolyan\Openai\ChatCompletion\CompletionRequest\ResponseFormatEnum;
use Tolyan\Openai\ChatCompletion\CompletionRequest\StreamOptions;
use Tolyan\Openai\ChatCompletion\CompletionRequest\ToolChoice;
use Tolyan\Openai\ChatCompletion\CompletionRequest\ToolInterface;

#[ClassSettings(
    renameWith: Cases::snake_case,
    omitNullFields: true,
)]
final class CompletionRequest
{
    /**
     * @param string                    $model             Required.
     *                                                     ID of the model to use. See the model endpoint compatibility
     *                                                     table for details on which models work with the Chat API.
     * @param array                     $messages          Required. A list of messages comprising the conversation so far.
     * @param float|null                $temperature       Optional. Defaults to 1.
     *                                                     What sampling temperature to use, between 0 and 2.
     *                                                     Higher values like 0.8 will make the output more random,
     *                                                     while lower values like 0.2 will make it more focused and deterministic.
     *                                                     We generally recommend altering this or top_p but not both.
     * @param int|null                  $maxTokens         Optional. The maximum number of tokens that can be generated
     *                                                     in the chat completion. The total length of input tokens and generated tokens
     *                                                     is limited by the model's context length.
     * @param float|null                $frequencyPenalty  Optional. Defaults to 0. Number between -2.0 and 2.0.
     *                                                     Positive values penalize new tokens based on their existing frequency in the text so far,
     *                                                     decreasing the model's likelihood to repeat the same line verbatim.
     *                                                     See more information about frequency and presence penalties.
     * @param array|null                $logitBias         Optional. Defaults to null. Modify the likelihood of specified tokens appearing
     *                                                     in the completion. Accepts a JSON object that maps tokens
     *                                                     (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
     *                                                     Mathematically, the bias is added to the logits generated by the model prior to sampling.
     *                                                     The exact effect will vary per model, but values between -1 and 1 should decrease or increase
     *                                                     likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection
     *                                                     of the relevant token.
     * @param bool|null                 $logprobs          Optional. Defaults to false. Whether to return log probabilities of the output
     *                                                     tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
     * @param int|null                  $topLogprobs       Optional. An integer between 0 and 20 specifying the number of most likely
     *                                                     tokens to return at each token position, each with an associated log probability.
     *                                                     logprobs must be set to true if this parameter is used.
     * @param int|null                  $n                 Optional. Defaults to 1. How many chat completion choices to generate for each input message.
     *                                                     Note that you will be charged based on the number of generated tokens across
     *                                                     all of the choices. Keep n as 1 to minimize costs.
     * @param float|null                $presencePenalty   Optional. Defaults to 0. Number between -2.0 and 2.0.
     *                                                     Positive values penalize new tokens based on whether they appear in the text so far,
     *                                                     increasing the model's likelihood to talk about new topics.
     *                                                     See more information about frequency and presence penalties.
     * @param ResponseFormatEnum|null   $responseFormat    Optional. An object specifying the format that the
     *                                                     model must output. Compatible with GPT-4o, GPT-4o mini, GPT-4 Turbo and all GPT-3.5 Turbo models
     *                                                     newer than gpt-3.5-turbo-1106. Setting to { "type": "json_object" } enables JSON mode,
     *                                                     which guarantees the message the model generates is valid JSON. Important: when using JSON mode,
     *                                                     you must also instruct the model to produce JSON yourself via a system or user message.
     *                                                     Without this, the model may generate an unending stream of whitespace until the generation
     *                                                     reaches the token limit, resulting in a long-running and seemingly "stuck" request.
     *                                                     Also note that the message content may be partially cut off if finish_reason="length",
     *                                                     which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
     * @param int|null                  $seed              Optional. This feature is in Beta. If specified, our system will make a best effort
     *                                                     to sample deterministically, such that repeated requests with the same seed and parameters
     *                                                     should return the same result. Determinism is not guaranteed, and you should refer to the
     *                                                     system_fingerprint response parameter to monitor changes in the backend.
     * @param string|null               $serviceTier       Optional. Defaults to null. Specifies the latency tier to use
     *                                                     for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
     *                                                     If set to 'auto', the system will utilize scale tier credits until they are exhausted.
     *                                                     If set to 'default', the request will be processed using the default service tier with a lower uptime
     *                                                     SLA and no latency guarentee. When not set, the default behavior is 'auto'.
     *                                                     When this parameter is set, the response body will include the service_tier utilized.
     * @param array|null                $stop              Optional. Defaults to null. Up to 4 sequences where
     *                                                     the API will stop generating further tokens.
     * @param bool|null                 $stream            Optional. Defaults to false. If set, partial message deltas will be sent,
     *                                                     like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available,
     *                                                     with the stream terminated by a data: [DONE] message.
     * @param StreamOptions|null        $streamOptions     Optional. Defaults to null. Options for streaming response.
     *                                                     Only set this when you set stream: true.
     * @param float|null                $topP              Optional. Defaults to 1. An alternative to sampling with temperature,
     *                                                     called nucleus sampling, where the model considers the results of the tokens
     *                                                     with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability
     *                                                     mass are considered. We generally recommend altering this or temperature but not both.
     * @param array<ToolInterface>|null $tools             Optional. A list of tools the model may call.
     *                                                     Currently, only functions are supported as a tool.
     *                                                     Use this to provide a list of functions the model may generate JSON inputs for.
     *                                                     A max of 128 functions are supported.
     * @param ToolChoice|null           $toolChoice        Optional. Controls which (if any) tool is called by the model.
     * @param bool|null                 $parallelToolCalls Optional. Defaults to true. Whether to enable parallel function calling
     *                                                     during tool use.
     * @param string|null               $user              Optional. A unique identifier representing your end-user,
     *                                                     which can help OpenAI to monitor and detect abuse.
     */
    public function __construct(
        public string $model,
        public array $messages,
        public ?float $temperature = null,
        public ?int $maxTokens = null,
        public ?float $frequencyPenalty = null,
        public ?array $logitBias = null,
        public ?bool $logprobs = null,
        public ?int $topLogprobs = null,
        public ?int $n = null,
        public ?float $presencePenalty = null,
        public ?ResponseFormatEnum $responseFormat = null,
        public ?int $seed = null,
        public ?string $serviceTier = null,
        public ?array $stop = null,
        public ?bool $stream = null,
        public ?StreamOptions $streamOptions = null,
        public ?float $topP = null,
        public ?array $tools = null,
        public ?ToolChoice $toolChoice = null,
        public ?bool $parallelToolCalls = null,
        public ?string $user = null,
    ) {
        $this->validate();
    }

    #[PostLoad]
    private function validate(): void
    {
        assert(
            $this->frequencyPenalty === null || (
                $this->frequencyPenalty >= -2.0 && $this->frequencyPenalty <= 2.0
            ),
            'frequencyPenalty must be between -2.0 and 2.0'
        );

        assert(
            $this->presencePenalty === null || (
                $this->presencePenalty >= -2.0 && $this->presencePenalty <= 2.0
            ),
            'presencePenalty must be between -2.0 and 2.0'
        );

        assert(
            $this->topP === null || ($this->topP >= 0.0 && $this->topP <= 1.0),
            'topP must be between 0.0 and 1.0'
        );

        assert(
            $this->temperature === null || ($this->temperature >= 0.0 && $this->temperature <= 2.0),
            'temperature must be between 0.0 and 2.0'
        );

        assert(
            $this->tools === null || count($this->tools) <= 128,
            'A max of 128 tools are supported.'
        );

        assert(
            $this->toolChoice === null || $this->tools !== null,
            'You must provide a list of tools the model may call.'
        );

        //        TODO:
        //        if ($this->toolChoice !== null && $this->tools !== null) {
        //            $toolNames = array_map(fn ($tool) => $tool::getName(), $this->tools);
        //            assert(
        //                in_array($this->toolChoice->tool, $toolNames),
        //                sprintf(
        //                    'ToolChoice tool %s must be one of the tools provided. Available tools: %s',
        //                    $this->toolChoice->tool,
        //                    implode(', ', $toolNames)
        //                )
        //            );
        //        }
    }
}